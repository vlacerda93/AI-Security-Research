# üîê AI Security Research

> Educational research on security aspects of Large Language Models and AI-generated content systems.

‚ö†Ô∏è **EDUCATIONAL PURPOSE ONLY / APENAS FINS EDUCACIONAIS**

This repository contains security analyses conducted for academic learning and cybersecurity skill development. All research follows responsible disclosure principles.

---

## üìö Studies

### [Gemini Policy Enforcement Inconsistency](./gemini-inconsistency/)

Analysis of session-based inconsistencies in Google Gemini's content filtering system, demonstrating:

- Session hopping bypass mechanisms
- Selective enforcement patterns  
- Deceptive failure modes
- Proposed mitigation strategies

**Status**: Educational documentation  
**Severity**: Medium-High (CVSS 9.1 base, adjusted for context)  
**Date**: December 2025

---

## üõ°Ô∏è Responsible Use

This research is intended to:
- Demonstrate security analysis skills
- Contribute to academic discussion on AI Safety
- Document learning process in cybersecurity

**Not recommended**:
- Large-scale exploitation attempts
- Malicious use for deepfakes or fraud  
- Testing without system owner consent

---

## üì¨ Contact

For responsible disclosure or collaboration:  
GitHub: [@vlacerda93](https://github.com/vlacerda93)

---

## üìÑ License

Content licensed under [MIT License](LICENSE) - feel free to reference with attribution.
